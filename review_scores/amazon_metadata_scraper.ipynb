{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scraper_tools' from '/Users/cheetham/Dropbox/insight/ideas/jigsaw/scraper_tools.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from scraper_tools import go_back,go_to_next_page,wait\n",
    "import scraper_tools\n",
    "from importlib import reload\n",
    "reload(scraper_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.amazon.com/s?i=toys-and-games&rh=n%3A165793011%2Cn%3A165795011%2Cn%3A166359011%2Cn%3A166363011%2Cp_n_feature_five_browse-bin%3A3136099011%2Cp_72%3A1248964011\"\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(base_url)\n",
    "wait(amount=10) # So I can change it to the US store..\n",
    "\n",
    "out_dir = '1000_piece/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?i=toys-and-games&rh=n%3A165793011%2Cn%3A165795011%2Cn%3A166359011%2Cn%3A166363011%2Cp_n_feature_five_browse-bin%3A3136099011%2Cp_72%3A1248964011&page=294\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2698a041db9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_url\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"&page=\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"results_page_{0:05}.html\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "\n",
    "# Main loop through search pages:\n",
    "for page_num in np.arange(0,0):\n",
    "    # Load the page\n",
    "    driver.get(base_url+\"&page=\"+str(page_num))\n",
    "    html = driver.page_source\n",
    "    print(base_url+\"&page=\"+str(page_num))\n",
    "    raise Exception\n",
    "    \n",
    "    fname = out_dir+\"results_page_{0:05}.html\".format(page_num)\n",
    "    with open(fname,'w') as myf:\n",
    "        myf.write(html)\n",
    "        \n",
    "    wait(amount=2)\n",
    "    print(' Done page {0}'.format(page_num))\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape the metadata from the saved pages\n",
    "urls = []\n",
    "for page_num in np.arange(1,294):\n",
    "    fname = out_dir+\"results_page_{0:05}.html\".format(page_num)\n",
    "    with open(fname,'r') as myf:\n",
    "        html = myf.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Get the product urls\n",
    "    results = soup.findAll('div',attrs={'class':'s-result-item'})\n",
    "\n",
    "    # Loop through\n",
    "    for item in results:\n",
    "        item_class = item.get('class')\n",
    "        # Skip the ads\n",
    "        if 'AdHolder' in item_class:\n",
    "            continue\n",
    "\n",
    "        # Get the product page url and save it for later\n",
    "        url = 'https://amazon.com'+item.find('a').get('href')\n",
    "        \n",
    "        # Strip off the trackers\n",
    "        url = url.rsplit('/',1)[0]\n",
    "        \n",
    "        urls.append(url)\n",
    "        \n",
    "# Save them all\n",
    "with open(out_dir+'product_urls.txt','w') as myf:\n",
    "    for url in urls:\n",
    "        myf.writelines(url+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(scraper_tools)\n",
    "# Now scrape each one\n",
    "with open(out_dir+'product_urls.txt','r') as myf:\n",
    "    product_urls = myf.readlines()\n",
    "\n",
    "# Set up the pandas array\n",
    "# reviews_df = scraper_tools.get_database()\n",
    "\n",
    "# reviews_df = reviews_df.drop([2360,2361,2362,2363,2364])\n",
    "\n",
    "# test=0\n",
    "# # Loop over product urls\n",
    "# for url_ix,product_url in enumerate(product_urls):    \n",
    "#     # Check if it's already filled:\n",
    "#     try:\n",
    "#         x = reviews_df['Name'][url_ix]\n",
    "#         if isinstance(reviews_df['Name'][url_ix],str):\n",
    "#             # Skip this one since it's already in the DB\n",
    "#             continue\n",
    "#         else:\n",
    "#             pass\n",
    "#     except:\n",
    "#         pass\n",
    "#     # Get the page metadata\n",
    "#     driver.get(product_url)\n",
    "    \n",
    "#     item_info = scraper_tools.get_info(driver)\n",
    "    \n",
    "#     n_saved = reviews_df.shape[0]-1\n",
    "    \n",
    "#     if n_saved > url_ix:\n",
    "#         for key in item_info.keys():\n",
    "#             reviews_df[key][url_ix] = item_info[key]\n",
    "#     else:\n",
    "#         reviews_df = reviews_df.append(item_info,ignore_index=True)\n",
    "    \n",
    "#     #Save it\n",
    "#     reviews_df.to_csv('review_db.csv')\n",
    "#     print(' Metadata for # {0} (saved as {1})'.format(url_ix,reviews_df.shape[0]-1))\n",
    "#     wait(amount=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(item_info)\n",
    "# reviews_df = reviews_df.append(item_info,ignore_index=True)\n",
    "# print(reviews_df.iloc[-1])\n",
    "reviews_df.to_csv('review_db.csv')\n",
    "# print(reviews_df.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Done!')\n",
    "# print(x)\n",
    "# fname = 'review_db.csv'\n",
    "# reviews_df = pd.read_csv(fname)\n",
    "# print(isinstance(reviews_df['Name'][url_ix],str))\n",
    "# reviews_df.to_csv('review_db2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Product 0: 92 pages\n"
     ]
    }
   ],
   "source": [
    "# Get reviews by replacing /dp/ with /product-reviews/ in the url!\n",
    "product_urls = np.loadtxt(out_dir+'product_urls.txt',dtype=str)\n",
    "\n",
    "# Set up the pandas array\n",
    "reviews_df = scraper_tools.get_database()\n",
    "\n",
    "for product_ix in np.arange(0,1):\n",
    "    \n",
    "    product_url = product_urls[product_ix]\n",
    "    \n",
    "    n_reviews = reviews_df['n_reviews'][product_ix]\n",
    "    if n_reviews != n_reviews:\n",
    "        n_reviews = '0 '\n",
    "    n_reviews = n_reviews.split(' ')[0]\n",
    "    n_reviews = int(n_reviews)\n",
    "    \n",
    "    n_pages = np.ceil(n_reviews/10).astype(int)\n",
    "    \n",
    "    all_ratings = []\n",
    "    all_reviews = []\n",
    "    \n",
    "    print('  Product {0}: {1} pages'.format(product_ix,n_pages))\n",
    "    \n",
    "    for page_ix in range(n_pages):\n",
    "        \n",
    "        # Navigate to page\n",
    "        review_url = product_url.replace('/dp/','/product-review/')\n",
    "        review_url += '/ref=cm_cr_arp_d_paging_btm_2?ie=UTF8&pageNumber={0}'.format(page_ix+1) # starts at 1\n",
    "        driver.get(review_url)\n",
    "        \n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        \n",
    "        # Loop through reviews on the page\n",
    "        reviews_on_page = soup.findAll('div',attrs={'data-hook':'review'})\n",
    "        for r in reviews_on_page:\n",
    "            # Get the star rating\n",
    "            rating_text = r.find('i',attrs={'data-hook':\"review-star-rating\"}).getText()\n",
    "            rating = float(rating_text[0:2])\n",
    "            all_ratings.append(rating)\n",
    "\n",
    "            # Get the review text\n",
    "            review_text = r.find('span',attrs={'data-hook':\"review-body\"}).getText()\n",
    "            all_reviews.append(review_text)\n",
    "        \n",
    "        wait(amount=1)\n",
    "        \n",
    "        product_df = pd.DataFrame({'ReviewRatings':all_ratings,'Reviews':all_reviews})\n",
    "        product_df.to_csv(out_dir+'product_{0:05}.csv'.format(product_ix))\n",
    "#         print(out_dir+'product_{0:05}.csv'.format(product_ix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6990\n"
     ]
    }
   ],
   "source": [
    "# print(len(all_ratings))\n",
    "# print(reviews_df['Reviews'][0:5].dtype)\n",
    "# print(n_reviews)\n",
    "print(len(product_urls))\n",
    "# print(n_pages)\n",
    "# print(reviews_df['n_reviews'][product_ix])\n",
    "# print(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
